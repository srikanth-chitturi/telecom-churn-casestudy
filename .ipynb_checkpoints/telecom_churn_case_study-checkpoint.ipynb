{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88476bfc",
   "metadata": {},
   "source": [
    "### Problem statement:-\n",
    "To reduce customer churn, telecom companies need to predict which customers are at high risk of churn.\n",
    "In this project, we will analyse customer-level data of a leading telecom firm, build predictive models to identify customers at high risk of churn and identify the main indicators of churn.\n",
    "\n",
    "Retaining high profitable customers is the main business goal here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf0c8d9",
   "metadata": {},
   "source": [
    "## Steps:-\n",
    "1. Reading, understanding and visualising the data\n",
    "2. Preparing the data for modelling\n",
    "3. Building the model\n",
    "4. Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9c63b046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5d268bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3059f7",
   "metadata": {},
   "source": [
    "##### Reading and understanding data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aac0c334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the dataset\n",
    "df = pd.read_csv('telecom_churn_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3f2cb292",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "549cebbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0fc53c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c47e253",
   "metadata": {},
   "source": [
    "#### Handling missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2ef7d0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values_count = df.isna().sum()\n",
    "\n",
    "# calculate the percentage of missing values in each column\n",
    "percentage_missing_values = round((missing_values_count / len(df)) * 100,2).to_frame('null').sort_values('null', ascending=False)\n",
    "\n",
    "# display the percentage of missing values in each column\n",
    "print(percentage_missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8f84bff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the columns having more than 30% missing values\n",
    "col_list_missing_30 = list(percentage_missing_values.index[percentage_missing_values['null'] > 30])\n",
    "print(col_list_missing_30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a134ed07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the columns having more than 30% missing values\n",
    "df = df.drop(col_list_missing_30, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c66825b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape after removing the missing values\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d431244a",
   "metadata": {},
   "source": [
    "##### Deleting the date columns as the date columns are not required in our analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fdcd77b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the date columns\n",
    "date_cols = [k for k in df.columns.to_list() if 'date' in k]\n",
    "print(date_cols) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "69a8c3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping date columns\n",
    "df = df.drop(date_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1ecedc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b6538e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop circle_id column as it is not necessary\n",
    "df = df.drop('circle_id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "33a23974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0805495c",
   "metadata": {},
   "source": [
    "#### Filter high-value customers\n",
    "It was mentioned we need to predict churn only for the high-value customers.High-value customers are mentioned as follows: Those who have recharged with an amount more than or equal to X, where X is the 70th percentile of the average recharge amount in the first two months (the good phase)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92846e3",
   "metadata": {},
   "source": [
    "Creating column `avg_rech_amt_6_7` by summing up total recharge amount of month 6 and 7. Then taking the average of the sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8d229e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['avg_rech_amt_6_7'] = (df['total_rech_amt_6'] + df['total_rech_amt_7'])/2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65c438a",
   "metadata": {},
   "source": [
    "Finding the 70th percentile of the avg_rech_amt_6_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f5b2a056",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['avg_rech_amt_6_7'].quantile(0.7)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8fcf3b",
   "metadata": {},
   "source": [
    "###### Filter the customers, who have recharged more than or equal to X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b341e4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['avg_rech_amt_6_7'] >= X]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c1e9d200",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00e2bc9",
   "metadata": {},
   "source": [
    "##### After filtering the high-value customers, we get about 30k rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faaf15b7",
   "metadata": {},
   "source": [
    "#### Handling missing values in rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a1815f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the rows having more than 50% missing values\n",
    "threshold = len(df.columns) / 2\n",
    "df_missing_rows_50 = df[(df.isnull().sum(axis=1)) > (threshold)]\n",
    "df_missing_rows_50.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fc0e5131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting the rows having more than 50% missing values\n",
    "df = df.drop(df_missing_rows_50.index)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c8c82233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the missing values in columns again\n",
    "df_missing_columns = (round(((df.isnull().sum()/len(df.index))*100),2).to_frame('null')).sort_values('null', ascending=False)\n",
    "df_missing_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2510e7dd",
   "metadata": {},
   "source": [
    "##### Looks like MOU for all the types of calls for the month of September (9) have missing values together for any particular record. Lets check the records for the MOU for Sep(9), in which these columns have missing values together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "17cc3e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing the columns of MOU Sep(9)\n",
    "print(((df_missing_columns[df_missing_columns['null'] == 5.32]).index).to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "93f36fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe with the condition, in which MOU for Sep(9) are null\n",
    "df_null_mou_9 = df[(df['loc_og_t2m_mou_9'].isnull()) & (df['loc_ic_t2f_mou_9'].isnull()) & (df['roam_og_mou_9'].isnull()) & (df['std_ic_t2m_mou_9'].isnull()) &\n",
    "  (df['loc_og_t2t_mou_9'].isnull()) & (df['std_ic_t2t_mou_9'].isnull()) & (df['loc_og_t2f_mou_9'].isnull()) & (df['loc_ic_mou_9'].isnull()) &\n",
    "  (df['loc_og_t2c_mou_9'].isnull()) & (df['loc_og_mou_9'].isnull()) & (df['std_og_t2t_mou_9'].isnull()) & (df['roam_ic_mou_9'].isnull()) &\n",
    "  (df['loc_ic_t2m_mou_9'].isnull()) & (df['std_og_t2m_mou_9'].isnull()) & (df['loc_ic_t2t_mou_9'].isnull()) & (df['std_og_t2f_mou_9'].isnull()) & \n",
    "  (df['std_og_t2c_mou_9'].isnull()) & (df['og_others_9'].isnull()) & (df['std_og_mou_9'].isnull()) & (df['spl_og_mou_9'].isnull()) & \n",
    "  (df['std_ic_t2f_mou_9'].isnull()) & (df['isd_og_mou_9'].isnull()) & (df['std_ic_mou_9'].isnull()) & (df['offnet_mou_9'].isnull()) & \n",
    "  (df['isd_ic_mou_9'].isnull()) & (df['ic_others_9'].isnull()) & (df['std_ic_t2o_mou_9'].isnull()) & (df['onnet_mou_9'].isnull()) & \n",
    "  (df['spl_ic_mou_9'].isnull())]\n",
    "\n",
    "df_null_mou_9.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "358db8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_null_mou_9.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6f4c9941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting the records for which MOU for Sep(9) are null\n",
    "df = df.drop(df_null_mou_9.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e2ec2705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again Checking percent of missing values in columns\n",
    "df_missing_columns = (round(((df.isnull().sum()/len(df.index))*100),2).to_frame('null')).sort_values('null', ascending=False)\n",
    "df_missing_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8251343d",
   "metadata": {},
   "source": [
    "##### Looks like MOU for all the types of calls for the month of Aug (8) have missing values together for any particular record. Lets check the records for the MOU for Aug(8), in which these columns have missing values together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "eda33014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing the columns of MOU Aug(8)\n",
    "print(((df_missing_columns[df_missing_columns['null'] == 0.55]).index).to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4294ecb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe with the condition, in which MOU for Aug(8) are null\n",
    "df_null_mou_8 = df[(df['loc_og_t2m_mou_8'].isnull()) & (df['loc_ic_t2f_mou_8'].isnull()) & (df['roam_og_mou_8'].isnull()) & (df['std_ic_t2m_mou_8'].isnull()) &\n",
    "  (df['loc_og_t2t_mou_8'].isnull()) & (df['std_ic_t2t_mou_8'].isnull()) & (df['loc_og_t2f_mou_8'].isnull()) & (df['loc_ic_mou_8'].isnull()) &\n",
    "  (df['loc_og_t2c_mou_8'].isnull()) & (df['loc_og_mou_8'].isnull()) & (df['std_og_t2t_mou_8'].isnull()) & (df['roam_ic_mou_8'].isnull()) &\n",
    "  (df['loc_ic_t2m_mou_8'].isnull()) & (df['std_og_t2m_mou_8'].isnull()) & (df['loc_ic_t2t_mou_8'].isnull()) & (df['std_og_t2f_mou_8'].isnull()) & \n",
    "  (df['std_og_t2c_mou_8'].isnull()) & (df['og_others_8'].isnull()) & (df['std_og_mou_8'].isnull()) & (df['spl_og_mou_8'].isnull()) & \n",
    "  (df['std_ic_t2f_mou_8'].isnull()) & (df['isd_og_mou_8'].isnull()) & (df['std_ic_mou_8'].isnull()) & (df['offnet_mou_8'].isnull()) & \n",
    "  (df['isd_ic_mou_8'].isnull()) & (df['ic_others_8'].isnull()) & (df['std_ic_t2o_mou_8'].isnull()) & (df['onnet_mou_8'].isnull()) & \n",
    "  (df['spl_ic_mou_8'].isnull())]\n",
    "\n",
    "df_null_mou_8.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b81f95cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting the records for which MOU for Aug(8) are null\n",
    "df = df.drop(df_null_mou_8.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4a99eb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again cheking percent of missing values in columns\n",
    "df_missing_columns = (round(((df.isnull().sum()/len(df.index))*100),2).to_frame('null')).sort_values('null', ascending=False)\n",
    "df_missing_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df5bb20",
   "metadata": {},
   "source": [
    "##### Looks like MOU for all the types of calls for the month of Jun (6) have missing values together for any particular record. Lets check the records for the MOU for Jun(6), in which these coulmns have missing values together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "865e677a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing the columns of MOU Jun(6)\n",
    "print(((df_missing_columns[df_missing_columns['null'] == 0.44]).index).to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2bdc8780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe with the condition, in which MOU for Jun(6) are null\n",
    "df_null_mou_6 = df[(df['loc_og_t2m_mou_6'].isnull()) & (df['loc_ic_t2f_mou_6'].isnull()) & (df['roam_og_mou_6'].isnull()) & (df['std_ic_t2m_mou_6'].isnull()) &\n",
    "  (df['loc_og_t2t_mou_6'].isnull()) & (df['std_ic_t2t_mou_6'].isnull()) & (df['loc_og_t2f_mou_6'].isnull()) & (df['loc_ic_mou_6'].isnull()) &\n",
    "  (df['loc_og_t2c_mou_6'].isnull()) & (df['loc_og_mou_6'].isnull()) & (df['std_og_t2t_mou_6'].isnull()) & (df['roam_ic_mou_6'].isnull()) &\n",
    "  (df['loc_ic_t2m_mou_6'].isnull()) & (df['std_og_t2m_mou_6'].isnull()) & (df['loc_ic_t2t_mou_6'].isnull()) & (df['std_og_t2f_mou_6'].isnull()) & \n",
    "  (df['std_og_t2c_mou_6'].isnull()) & (df['og_others_6'].isnull()) & (df['std_og_mou_6'].isnull()) & (df['spl_og_mou_6'].isnull()) & \n",
    "  (df['std_ic_t2f_mou_6'].isnull()) & (df['isd_og_mou_6'].isnull()) & (df['std_ic_mou_6'].isnull()) & (df['offnet_mou_6'].isnull()) & \n",
    "  (df['isd_ic_mou_6'].isnull()) & (df['ic_others_6'].isnull()) & (df['std_ic_t2o_mou_6'].isnull()) & (df['onnet_mou_6'].isnull()) & \n",
    "  (df['spl_ic_mou_6'].isnull())]\n",
    "\n",
    "df_null_mou_6.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4c3aac33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting the records for which MOU for Jun(6) are null\n",
    "df = df.drop(df_null_mou_6.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c0805995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again cheking percent of missing values in columns\n",
    "df_missing_columns = (round(((df.isnull().sum()/len(df.index))*100),2).to_frame('null')).sort_values('null', ascending=False)\n",
    "df_missing_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e147597b",
   "metadata": {},
   "source": [
    "###### Looks like MOU for all the types of calls for the month of July (7) have missing values together for any particular record. Lets check the records for the MOU for Jul(7), in which these coulmns have missing values together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0dd0325d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing the columns of MOU Jul(7)\n",
    "print(((df_missing_columns[df_missing_columns['null'] == 0.12]).index).to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8305dc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe with the condition, in which MOU for Jul(7) are null\n",
    "df_null_mou_7 = df[(df['loc_og_t2m_mou_7'].isnull()) & (df['loc_ic_t2f_mou_7'].isnull()) & (df['roam_og_mou_7'].isnull()) & (df['std_ic_t2m_mou_7'].isnull()) &\n",
    "  (df['loc_og_t2t_mou_7'].isnull()) & (df['std_ic_t2t_mou_7'].isnull()) & (df['loc_og_t2f_mou_7'].isnull()) & (df['loc_ic_mou_7'].isnull()) &\n",
    "  (df['loc_og_t2c_mou_7'].isnull()) & (df['loc_og_mou_7'].isnull()) & (df['std_og_t2t_mou_7'].isnull()) & (df['roam_ic_mou_7'].isnull()) &\n",
    "  (df['loc_ic_t2m_mou_7'].isnull()) & (df['std_og_t2m_mou_7'].isnull()) & (df['loc_ic_t2t_mou_7'].isnull()) & (df['std_og_t2f_mou_7'].isnull()) & \n",
    "  (df['std_og_t2c_mou_7'].isnull()) & (df['og_others_7'].isnull()) & (df['std_og_mou_7'].isnull()) & (df['spl_og_mou_7'].isnull()) & \n",
    "  (df['std_ic_t2f_mou_7'].isnull()) & (df['isd_og_mou_7'].isnull()) & (df['std_ic_mou_7'].isnull()) & (df['offnet_mou_7'].isnull()) & \n",
    "  (df['isd_ic_mou_7'].isnull()) & (df['ic_others_7'].isnull()) & (df['std_ic_t2o_mou_7'].isnull()) & (df['onnet_mou_7'].isnull()) & \n",
    "  (df['spl_ic_mou_7'].isnull())]\n",
    "\n",
    "df_null_mou_7.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7d4f3234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting the records for which MOU for Jul(7) are null\n",
    "df = df.drop(df_null_mou_7.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5c58e0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again cheking percent of missing values in columns\n",
    "df_missing_columns = (round(((df.isnull().sum()/len(df.index))*100),2).to_frame('null')).sort_values('null', ascending=False)\n",
    "df_missing_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28576d0",
   "metadata": {},
   "source": [
    "######  We can observe there are no missing values in any columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a4c8038f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1b295f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking percentage of rows we have lost while handling the missing values\n",
    "round((1- (len(df.index)/30011)),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86233b2c",
   "metadata": {},
   "source": [
    "##### We can see that we have lost almost 7% records. But we have enough number of records to do our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8234af60",
   "metadata": {},
   "source": [
    "### Tag churners"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613b29f7",
   "metadata": {},
   "source": [
    "##### Now tag the churned customers (churn=1, else 0) based on the fourth month as follows: Those who have not made any calls (either incoming or outgoing) AND have not used mobile internet even once in the churn phase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb20335a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['churn'] = np.where((df['total_ic_mou_9']==0) & (df['total_og_mou_9']==0) & (df['vol_2g_mb_9']==0) & (df['vol_3g_mb_9']==0), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c77809e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f60259",
   "metadata": {},
   "source": [
    "#### Deleting all the attributes corresponding to the churn phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59603bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the columns for churn month(9)\n",
    "col_9 = [col for col in df.columns.to_list() if '_9' in col]\n",
    "print(col_9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513dd4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting the churn month columns\n",
    "df = df.drop(col_9, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97faebe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping sep_vbc_3g column\n",
    "df = df.drop('sep_vbc_3g', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b3d064",
   "metadata": {},
   "source": [
    "#### Checking churn percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2effa79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "round(100*(df['churn'].mean()),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa4c219",
   "metadata": {},
   "source": [
    "##### There is very little percentage of churn rate. We will take care of the class imbalance later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d72592",
   "metadata": {},
   "source": [
    "## Outliers treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4ee383",
   "metadata": {},
   "source": [
    "In the filtered dataset except mobile_number and churn columns all the columns are numeric types. Hence, converting mobile_number and churn datatype to object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db88136",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['mobile_number'] = df['mobile_number'].astype(object)\n",
    "df['churn'] = df['churn'].astype(object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbd216f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3e3e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List only the numeric columns\n",
    "numeric_cols = df.select_dtypes(exclude=['object']).columns\n",
    "print(numeric_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d48112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing outliers below 10th and above 90th percentile\n",
    "for col in numeric_cols: \n",
    "    q1 = df[col].quantile(0.10)\n",
    "    q3 = df[col].quantile(0.90)\n",
    "    iqr = q3-q1\n",
    "    range_low  = q1-1.5*iqr\n",
    "    range_high = q3+1.5*iqr\n",
    "    # Assigning the filtered dataset into data\n",
    "    data = df.loc[(df[col] > range_low) & (df[col] < range_high)]\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff60906a",
   "metadata": {},
   "source": [
    "### Derive new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048b50b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the columns of total mou, rech_num and rech_amt\n",
    "[total for total in data.columns.to_list() if 'total' in total]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a788200",
   "metadata": {},
   "source": [
    "#### Deriving new column `decrease_mou_action`\n",
    "This column indicates whether the minutes of usage of the customer has decreased in the action phase than the good phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a50de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total mou at good phase incoming and outgoing\n",
    "data['total_mou_good'] = (data['total_og_mou_6'] + data['total_ic_mou_6'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9e7486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avg. mou at action phase\n",
    "# We are taking average because there are two months(7 and 8) in action phase\n",
    "data['avg_mou_action'] = (data['total_og_mou_7'] + data['total_og_mou_8'] + data['total_ic_mou_7'] + data['total_ic_mou_8'])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74842fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Difference avg_mou_good and avg_mou_action\n",
    "data['diff_mou'] = data['avg_mou_action'] - data['total_mou_good']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddcb2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking whether the mou has decreased in action phase\n",
    "data['decrease_mou_action'] = np.where((data['diff_mou'] < 0), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff730d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7849fb9f",
   "metadata": {},
   "source": [
    "#### Deriving new column `decrease_rech_num_action`\n",
    "This column indicates whether the number of recharge of the customer has decreased in the action phase than the good phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e0dec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avg recharge number at action phase\n",
    "data['avg_rech_num_action'] = (data['total_rech_num_7'] + data['total_rech_num_8'])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6145644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Difference total_rech_num_6 and avg_rech_action\n",
    "data['diff_rech_num'] = data['avg_rech_num_action'] - data['total_rech_num_6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f8d38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if rech_num has decreased in action phase\n",
    "data['decrease_rech_num_action'] = np.where((data['diff_rech_num'] < 0), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d944cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df41ee5",
   "metadata": {},
   "source": [
    "#### Deriving new column `decrease_rech_amt_action`\n",
    "This column indicates whether the amount of recharge of the customer has decreased in the action phase than the good phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20f2106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avg rech_amt in action phase\n",
    "data['avg_rech_amt_action'] = (data['total_rech_amt_7'] + data['total_rech_amt_8'])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d25c342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Difference of action phase rech amt and good phase rech amt\n",
    "data['diff_rech_amt'] = data['avg_rech_amt_action'] - data['total_rech_amt_6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdc8184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if rech_amt has decreased in action phase\n",
    "data['decrease_rech_amt_action'] = np.where((data['diff_rech_amt'] < 0), 1, 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5642c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c24a8f",
   "metadata": {},
   "source": [
    "#### Deriving new column `decrease_arpu_action`\n",
    "This column indicates whether the average revenue per customer has decreased in the action phase than the good phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d531d9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARUP in action phase\n",
    "data['avg_arpu_action'] = (data['arpu_7'] + data['arpu_8'])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1f97d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Difference of good and action phase ARPU\n",
    "data['diff_arpu'] = data['avg_arpu_action'] - data['arpu_6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39310a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking whether the arpu has decreased on the action month\n",
    "data['decrease_arpu_action'] = np.where(data['diff_arpu'] < 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2f8c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbaab8b7",
   "metadata": {},
   "source": [
    "#### Deriving new column `decrease_vbc_action`\n",
    "This column indicates whether the volume based cost of the customer has decreased in the action phase than the good phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2682ba9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VBC in action phase\n",
    "data['avg_vbc_3g_action'] = (data['jul_vbc_3g'] + data['aug_vbc_3g'])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fb09b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Difference of good and action phase VBC\n",
    "data['diff_vbc'] = data['avg_vbc_3g_action'] - data['jun_vbc_3g']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7ebd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking whether the VBC has decreased on the action month\n",
    "data['decrease_vbc_action'] = np.where(data['diff_vbc'] < 0 , 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0f236c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c19478",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffa623c",
   "metadata": {},
   "source": [
    "### Univariate analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ed7925",
   "metadata": {},
   "source": [
    "##### Churn rate on the basis whether the customer decreased her/his MOU in action month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235ddbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting churn column to int in order to do aggfunc in the pivot table\n",
    "data['churn'] = data['churn'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf933dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.pivot_table(values='churn', index='decrease_mou_action', aggfunc='mean').plot.bar()\n",
    "plt.ylabel('churn rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e927a27",
   "metadata": {},
   "source": [
    "***Analysis***\n",
    "\n",
    "We can see that the churn rate is more for the customers, whose minutes of usage(mou) decreased in the action phase than the good phase. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c08e7d",
   "metadata": {},
   "source": [
    "##### Churn rate on the basis whether the customer decreased her/his number of recharge in action month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db22b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.pivot_table(values='churn', index='decrease_rech_num_action', aggfunc='mean').plot.bar()\n",
    "plt.ylabel('churn rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f20181",
   "metadata": {},
   "source": [
    "***Analysis***\n",
    "\n",
    "As expected, the churn rate is more for the customers, whose number of recharge in the action phase is lesser than the number in good phase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d25e9c0",
   "metadata": {},
   "source": [
    "##### Churn rate on the basis whether the customer decreased her/his amount of recharge in action month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7f559e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.pivot_table(values='churn', index='decrease_rech_amt_action', aggfunc='mean').plot.bar()\n",
    "plt.ylabel('churn rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba25304",
   "metadata": {},
   "source": [
    "***Analysis***\n",
    "\n",
    "Here also we see the same behaviour. The churn rate is more for the customers, whose amount of recharge in the action phase is lesser than the amount in good phase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02ff0e4",
   "metadata": {},
   "source": [
    "##### Churn rate on the basis whether the customer decreased her/his volume based cost in action month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb50b742",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.pivot_table(values='churn', index='decrease_vbc_action', aggfunc='mean').plot.bar()\n",
    "plt.ylabel('churn rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8d3576",
   "metadata": {},
   "source": [
    "***Analysis***\n",
    "\n",
    "Here we see the expected result. The churn rate is more for the customers, whose volume based cost in action month is increased. That means the customers do not do the monthly recharge more when they are in the action phase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97aa4fa",
   "metadata": {},
   "source": [
    "##### Analysis of the average revenue per customer (churn and not churn) in the action phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a10d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating churn dataframe\n",
    "data_churn = data[data['churn'] == 1]\n",
    "# Creating not churn dataframe\n",
    "data_non_churn = data[data['churn'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd5ecf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution plot\n",
    "ax = sns.distplot(data_churn['avg_arpu_action'],label='churn',hist=False)\n",
    "ax = sns.distplot(data_non_churn['avg_arpu_action'],label='not churn',hist=False)\n",
    "ax.set(xlabel='Action phase ARPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019337b3",
   "metadata": {},
   "source": [
    "Average revenue per user (ARPU) for the churned customers is mostly densed on the 0 to 900. The higher ARPU customers are less likely to be churned.\n",
    "\n",
    "ARPU for the not churned customers is mostly densed on the 0 to 1000. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7412b96",
   "metadata": {},
   "source": [
    "##### Analysis of the minutes of usage MOU (churn and not churn) in the action phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b126fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution plot\n",
    "ax = sns.distplot(data_churn['total_mou_good'],label='churn',hist=False)\n",
    "ax = sns.distplot(data_non_churn['total_mou_good'],label='non churn',hist=False)\n",
    "ax.set(xlabel='Action phase MOU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c903d70",
   "metadata": {},
   "source": [
    "Minutes of usage(MOU) of the churn customers is mostly populated on the 0 to 2500 range. Higher the MOU, lesser the churn probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58361a9f",
   "metadata": {},
   "source": [
    "### Bivariate analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d331a07",
   "metadata": {},
   "source": [
    "##### Analysis of churn rate by the decreasing recharge amount and number of recharge in the action phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74832d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.pivot_table(values='churn', index='decrease_rech_amt_action', columns='decrease_rech_num_action', aggfunc='mean').plot.bar()\n",
    "plt.ylabel('churn rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6ae95a",
   "metadata": {},
   "source": [
    "***Analysis***\n",
    "\n",
    "We can see from the above plot, that the churn rate is more for the customers, whose recharge amount as well as number of recharge have decreased in the action phase than the good phase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f879b9d",
   "metadata": {},
   "source": [
    "##### Analysis of churn rate by the decreasing recharge amount and volume based cost in the action phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac0015d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.pivot_table(values='churn', index='decrease_rech_amt_action', columns='decrease_vbc_action', aggfunc='mean').plot.bar()\n",
    "plt.ylabel('churn rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b673dcb",
   "metadata": {},
   "source": [
    "***Analysis***\n",
    "\n",
    "Here, also we can see that the churn rate is more for the customers, whose recharge amount is decreased along with the volume based cost is increased in the action month."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46988026",
   "metadata": {},
   "source": [
    "##### Analysis of recharge amount and number of recharge in action month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18e71a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "ax = sns.scatterplot('avg_rech_num_action','avg_rech_amt_action', hue='churn', data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a18840c",
   "metadata": {},
   "source": [
    "***Analysis***\n",
    "\n",
    "We can see from the above pattern that the recharge number and the recharge amount are mostly propotional. More the number of recharge, more the amount of the recharge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277a9314",
   "metadata": {},
   "source": [
    "#### Dropping few derived columns, which are not required in further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77596d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['total_mou_good','avg_mou_action','diff_mou','avg_rech_num_action','diff_rech_num','avg_rech_amt_action',\n",
    "                 'diff_rech_amt','avg_arpu_action','diff_arpu','avg_vbc_3g_action','diff_vbc','avg_rech_amt_6_7'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab8eb5b",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bccfa31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import sklearn library\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad8c2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting feature variables into X\n",
    "X = data.drop(['mobile_number','churn'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a658f32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting target variable to y\n",
    "y = data['churn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4851e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into train and test set 80:20\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b51ff44",
   "metadata": {},
   "source": [
    "### Dealing with data imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfc94c8",
   "metadata": {},
   "source": [
    "We are creating synthetic samples by doing upsampling using SMOTE(Synthetic Minority Oversampling Technique)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2983f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imporing SMOTE\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64479bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate SMOTE\n",
    "sm = SMOTE(random_state=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b262adb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fittign SMOTE to the train set\n",
    "X_train, y_train = sm.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0354ec",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6888ca87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization method\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2540c846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Scaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d551bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of the numeric columns\n",
    "cols_scale = X_train.columns.to_list()\n",
    "# Removing the derived binary columns \n",
    "cols_scale.remove('decrease_mou_action')\n",
    "cols_scale.remove('decrease_rech_num_action')\n",
    "cols_scale.remove('decrease_rech_amt_action')\n",
    "cols_scale.remove('decrease_arpu_action')\n",
    "cols_scale.remove('decrease_vbc_action')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faca67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the data into scaler and transform\n",
    "X_train[cols_scale] = scaler.fit_transform(X_train[cols_scale])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13ab4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06761ff6",
   "metadata": {},
   "source": [
    "##### Scaling the test set\n",
    "We don't fit scaler on the test set. We only transform the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56d2713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the test set\n",
    "X_test[cols_scale] = scaler.transform(X_test[cols_scale])\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea20633",
   "metadata": {},
   "source": [
    "# Model with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbffa919",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import PCA\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b97d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate PCA\n",
    "pca = PCA(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1c2cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit train set on PCA\n",
    "pca.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5654a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Principal components\n",
    "pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850e7bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cumuliative varinace of the PCs\n",
    "variance_cumu = np.cumsum(pca.explained_variance_ratio_)\n",
    "print(variance_cumu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb02bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting scree plot\n",
    "fig = plt.figure(figsize = (10,6))\n",
    "plt.plot(variance_cumu)\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Variance')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49e3f19",
   "metadata": {},
   "source": [
    "We can see that `60 components` explain amost more than 90% variance of the data. So, we will perform PCA with 60 components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75de90e9",
   "metadata": {},
   "source": [
    "##### Performing PCA with 60 components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f714a794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing incremental PCA\n",
    "from sklearn.decomposition import IncrementalPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406f3348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate PCA with 60 components\n",
    "pca_final = IncrementalPCA(n_components=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba73c096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform the X_train\n",
    "X_train_pca = pca_final.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b902b2",
   "metadata": {},
   "source": [
    "##### Applying transformation on the test set\n",
    "We are only doing Transform in the test set not the Fit-Transform. Because the Fitting is already done on the train set. So, we just have to do the transformation with the already fitted data on the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fb9b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_pca = pca_final.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01837576",
   "metadata": {},
   "source": [
    "#### Emphasize Sensitivity/Recall than Accuracy\n",
    "\n",
    "We are more focused on higher Sensitivity/Recall score than the accuracy.\n",
    "\n",
    "Beacuse we need to care more about churn cases than the not churn cases. The main goal is to reatin the customers, who have the possiblity to churn. There should not be a problem, if we consider few not churn customers as churn customers and provide them some incentives for retaining them. Hence, the sensitivity score is more important here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc73ef9",
   "metadata": {},
   "source": [
    "## Logistic regression with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a60c8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing scikit logistic regression module\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8028d992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impoting metrics\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd87216",
   "metadata": {},
   "source": [
    "#### Tuning hyperparameter C\n",
    "C is the the inverse of regularization strength in Logistic Regression. Higher values of C correspond to less regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83a98d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries for cross validation\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54462b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating KFold object with 5 splits\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=4)\n",
    "\n",
    "# Specify params\n",
    "params = {\"C\": [0.01, 0.1, 1, 10, 100, 1000]}\n",
    "\n",
    "# Specifing score as recall as we are more focused on acheiving the higher sensitivity than the accuracy\n",
    "model_cv = GridSearchCV(estimator = LogisticRegression(),\n",
    "                        param_grid = params, \n",
    "                        scoring= 'recall', \n",
    "                        cv = folds, \n",
    "                        verbose = 1,\n",
    "                        return_train_score=True) \n",
    "\n",
    "# Fit the model\n",
    "model_cv.fit(X_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8f9cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results of grid search CV\n",
    "cv_results = pd.DataFrame(model_cv.cv_results_)\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5231838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot of C versus train and validation scores\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(cv_results['param_C'], cv_results['mean_test_score'])\n",
    "plt.plot(cv_results['param_C'], cv_results['mean_train_score'])\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('sensitivity')\n",
    "plt.legend(['test result', 'train result'], loc='upper left')\n",
    "plt.xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e89975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best score with best C\n",
    "best_score = model_cv.best_score_\n",
    "best_C = model_cv.best_params_['C']\n",
    "\n",
    "print(\" The highest test sensitivity is {0} at C = {1}\".format(best_score, best_C))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b7ba1c",
   "metadata": {},
   "source": [
    "#### Logistic regression with optimal C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040e09f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model with best C\n",
    "logistic_pca = LogisticRegression(C=best_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6d0aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model on the train set\n",
    "log_pca_model = logistic_pca.fit(X_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5f5ce2",
   "metadata": {},
   "source": [
    "##### Prediction on the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e10e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on the train set\n",
    "y_train_pred = log_pca_model.predict(X_train_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711aea04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "confusion = metrics.confusion_matrix(y_train, y_train_pred)\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d978a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = confusion[1,1] # true positive \n",
    "TN = confusion[0,0] # true negatives\n",
    "FP = confusion[0,1] # false positives\n",
    "FN = confusion[1,0] # false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e5cfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "print(\"Accuracy:-\",metrics.accuracy_score(y_train, y_train_pred))\n",
    "\n",
    "# Sensitivity\n",
    "print(\"Sensitivity:-\",TP / float(TP+FN))\n",
    "\n",
    "# Specificity\n",
    "print(\"Specificity:-\", TN / float(TN+FP))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b486949",
   "metadata": {},
   "source": [
    "##### Prediction on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec715f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on the test set\n",
    "y_test_pred = log_pca_model.predict(X_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56716586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "confusion = metrics.confusion_matrix(y_test, y_test_pred)\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dd4ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = confusion[1,1] # true positive \n",
    "TN = confusion[0,0] # true negatives\n",
    "FP = confusion[0,1] # false positives\n",
    "FN = confusion[1,0] # false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fbd314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "print(\"Accuracy:-\",metrics.accuracy_score(y_test, y_test_pred))\n",
    "\n",
    "# Sensitivity\n",
    "print(\"Sensitivity:-\",TP / float(TP+FN))\n",
    "\n",
    "# Specificity\n",
    "print(\"Specificity:-\", TN / float(TN+FP))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5799282c",
   "metadata": {},
   "source": [
    "***Model summary***\n",
    "\n",
    "- Train set\n",
    "    - Accuracy = 0.86\n",
    "    - Sensitivity = 0.89\n",
    "    - Specificity = 0.83\n",
    "- Test set\n",
    "    - Accuracy = 0.83\n",
    "    - Sensitivity = 0.81\n",
    "    - Specificity = 0.83\n",
    "    \n",
    "Overall, the model is performing well in the test set, what it had learnt from the train set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfeff710",
   "metadata": {},
   "source": [
    "## Support Vector Machine(SVM) with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328c506f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing SVC\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0bc003",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning\n",
    "\n",
    "C:- Regularization parameter.\n",
    "\n",
    "gamma:- Handles non linear classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba33664e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify range of hyperparameters\n",
    "\n",
    "hyper_params = [ {'gamma': [1e-2, 1e-3, 1e-4],\n",
    "                     'C': [1, 10, 100, 1000]}]\n",
    "\n",
    "\n",
    "# specify model with RBF kernel\n",
    "model = SVC(kernel=\"rbf\")\n",
    "\n",
    "# set up GridSearchCV()\n",
    "model_cv = GridSearchCV(estimator = model, \n",
    "                        param_grid = hyper_params, \n",
    "                        scoring= 'accuracy', \n",
    "                        cv = 3, \n",
    "                        verbose = 1,\n",
    "                        return_train_score=True)      \n",
    "\n",
    "# fit the model\n",
    "model_cv.fit(X_train_pca, y_train)                  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6cffb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv results\n",
    "cv_results = pd.DataFrame(model_cv.cv_results_)\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c61c348",
   "metadata": {},
   "source": [
    "##### Plotting the accuracy with various C and gamma values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee03696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting C to numeric type for plotting on x-axis\n",
    "cv_results['param_C'] = cv_results['param_C'].astype('int')\n",
    "\n",
    "# # plotting\n",
    "plt.figure(figsize=(16,6))\n",
    "\n",
    "# subplot 1/3\n",
    "plt.subplot(131)\n",
    "gamma_01 = cv_results[cv_results['param_gamma']==0.01]\n",
    "\n",
    "plt.plot(gamma_01[\"param_C\"], gamma_01[\"mean_test_score\"])\n",
    "plt.plot(gamma_01[\"param_C\"], gamma_01[\"mean_train_score\"])\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title(\"Gamma=0.01\")\n",
    "plt.ylim([0.80, 1])\n",
    "plt.legend(['test accuracy', 'train accuracy'], loc='upper left')\n",
    "plt.xscale('log')\n",
    "\n",
    "# subplot 2/3\n",
    "plt.subplot(132)\n",
    "gamma_001 = cv_results[cv_results['param_gamma']==0.001]\n",
    "\n",
    "plt.plot(gamma_001[\"param_C\"], gamma_001[\"mean_test_score\"])\n",
    "plt.plot(gamma_001[\"param_C\"], gamma_001[\"mean_train_score\"])\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title(\"Gamma=0.001\")\n",
    "plt.ylim([0.80, 1])\n",
    "plt.legend(['test accuracy', 'train accuracy'], loc='upper left')\n",
    "plt.xscale('log')\n",
    "\n",
    "\n",
    "# subplot 3/3\n",
    "plt.subplot(133)\n",
    "gamma_0001 = cv_results[cv_results['param_gamma']==0.0001]\n",
    "\n",
    "plt.plot(gamma_0001[\"param_C\"], gamma_0001[\"mean_test_score\"])\n",
    "plt.plot(gamma_0001[\"param_C\"], gamma_0001[\"mean_train_score\"])\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title(\"Gamma=0.0001\")\n",
    "plt.ylim([0.80, 1])\n",
    "plt.legend(['test accuracy', 'train accuracy'], loc='upper left')\n",
    "plt.xscale('log')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d09421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the best score \n",
    "best_score = model_cv.best_score_\n",
    "best_hyperparams = model_cv.best_params_\n",
    "\n",
    "print(\"The best test score is {0} corresponding to hyperparameters {1}\".format(best_score, best_hyperparams))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee80d622",
   "metadata": {},
   "source": [
    "From the above plot, we can see that higher value of gamma leads to overfitting the model. With the lowest value of gamma (0.0001) we have train and test accuracy almost same.\n",
    "\n",
    "Also, at C=100 we have a good accuracy and the train and test scores are comparable.\n",
    "\n",
    "Though sklearn suggests the optimal scores mentioned above (gamma=0.01, C=1000), one could argue that it is better to choose a simpler, more non-linear model with gamma=0.0001. This is because the optimal values mentioned here are calculated based on the average test accuracy (but not considering subjective parameters such as model complexity).\n",
    "\n",
    "We can achieve comparable average test accuracy (~90%) with gamma=0.0001 as well, though we'll have to increase the cost C for that. So to achieve high accuracy, there's a tradeoff between:\n",
    "- High gamma (i.e. high non-linearity) and average value of C\n",
    "- Low gamma (i.e. less non-linearity) and high value of C\n",
    "\n",
    "We argue that the model will be simpler if it has as less non-linearity as possible, so we choose gamma=0.0001 and a high C=100."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf0f18c",
   "metadata": {},
   "source": [
    "##### Build the model with optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50df0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the model with optimal hyperparameters\n",
    "svm_pca_model = SVC(C=100, gamma=0.0001, kernel=\"rbf\")\n",
    "\n",
    "svm_pca_model.fit(X_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba10c91",
   "metadata": {},
   "source": [
    "##### Prediction on the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3230b5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on the train set\n",
    "y_train_pred = svm_pca_model.predict(X_train_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72db5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "confusion = metrics.confusion_matrix(y_train, y_train_pred)\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b009cd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = confusion[1,1] # true positive \n",
    "TN = confusion[0,0] # true negatives\n",
    "FP = confusion[0,1] # false positives\n",
    "FN = confusion[1,0] # false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a9f920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "print(\"Accuracy:-\",metrics.accuracy_score(y_train, y_train_pred))\n",
    "\n",
    "# Sensitivity\n",
    "print(\"Sensitivity:-\",TP / float(TP+FN))\n",
    "\n",
    "# Specificity\n",
    "print(\"Specificity:-\", TN / float(TN+FP))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3cfbfa",
   "metadata": {},
   "source": [
    "##### Prediction on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6876aecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on the test set\n",
    "y_test_pred = svm_pca_model.predict(X_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfea97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "confusion = metrics.confusion_matrix(y_test, y_test_pred)\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a8304b",
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = confusion[1,1] # true positive \n",
    "TN = confusion[0,0] # true negatives\n",
    "FP = confusion[0,1] # false positives\n",
    "FN = confusion[1,0] # false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcaace08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "print(\"Accuracy:-\",metrics.accuracy_score(y_test, y_test_pred))\n",
    "\n",
    "# Sensitivity\n",
    "print(\"Sensitivity:-\",TP / float(TP+FN))\n",
    "\n",
    "# Specificity\n",
    "print(\"Specificity:-\", TN / float(TN+FP))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745a6641",
   "metadata": {},
   "source": [
    "***Model summary***\n",
    "\n",
    "- Train set\n",
    "    - Accuracy = 0.89\n",
    "    - Sensitivity = 0.92\n",
    "    - Specificity = 0.85\n",
    "- Test set\n",
    "    - Accuracy = 0.85\n",
    "    - Sensitivity = 0.81\n",
    "    - Specificity = 0.85"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccbdfd9",
   "metadata": {},
   "source": [
    "## Decision tree with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa82248f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing decision tree classifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fc082b",
   "metadata": {},
   "source": [
    "##### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f448bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parameter grid \n",
    "param_grid = {\n",
    "    'max_depth': range(5, 15, 5),\n",
    "    'min_samples_leaf': range(50, 150, 50),\n",
    "    'min_samples_split': range(50, 150, 50),\n",
    "}\n",
    "\n",
    "\n",
    "# Instantiate the grid search model\n",
    "dtree = DecisionTreeClassifier()\n",
    "\n",
    "grid_search = GridSearchCV(estimator = dtree, \n",
    "                           param_grid = param_grid, \n",
    "                           scoring= 'recall',\n",
    "                           cv = 5, \n",
    "                           verbose = 1)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train_pca,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8adf3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv results\n",
    "cv_results = pd.DataFrame(grid_search.cv_results_)\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542fcfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the optimal sensitivity score and hyperparameters\n",
    "print(\"Best sensitivity:-\", grid_search.best_score_)\n",
    "print(grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c40b7b0",
   "metadata": {},
   "source": [
    "##### Model with optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fa3146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with optimal hyperparameters\n",
    "dt_pca_model = DecisionTreeClassifier(criterion = \"gini\", \n",
    "                                  random_state = 100,\n",
    "                                  max_depth=10, \n",
    "                                  min_samples_leaf=50,\n",
    "                                  min_samples_split=50)\n",
    "\n",
    "dt_pca_model.fit(X_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6908fc67",
   "metadata": {},
   "source": [
    "##### Prediction on the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59ee654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on the train set\n",
    "y_train_pred = dt_pca_model.predict(X_train_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad029ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "confusion = metrics.confusion_matrix(y_train, y_train_pred)\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2abb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = confusion[1,1] # true positive \n",
    "TN = confusion[0,0] # true negatives\n",
    "FP = confusion[0,1] # false positives\n",
    "FN = confusion[1,0] # false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf92d3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "print(\"Accuracy:-\",metrics.accuracy_score(y_train, y_train_pred))\n",
    "\n",
    "# Sensitivity\n",
    "print(\"Sensitivity:-\",TP / float(TP+FN))\n",
    "\n",
    "# Specificity\n",
    "print(\"Specificity:-\", TN / float(TN+FP))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cccd4a",
   "metadata": {},
   "source": [
    "##### Prediction on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c50018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on the test set\n",
    "y_test_pred = dt_pca_model.predict(X_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acddc1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "confusion = metrics.confusion_matrix(y_test, y_test_pred)\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564e8459",
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = confusion[1,1] # true positive \n",
    "TN = confusion[0,0] # true negatives\n",
    "FP = confusion[0,1] # false positives\n",
    "FN = confusion[1,0] # false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af1367d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "print(\"Accuracy:-\",metrics.accuracy_score(y_test, y_test_pred))\n",
    "\n",
    "# Sensitivity\n",
    "print(\"Sensitivity:-\",TP / float(TP+FN))\n",
    "\n",
    "# Specificity\n",
    "print(\"Specificity:-\", TN / float(TN+FP))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d84356",
   "metadata": {},
   "source": [
    "***Model summary***\n",
    "\n",
    "- Train set\n",
    "    - Accuracy = 0.90\n",
    "    - Sensitivity = 0.91\n",
    "    - Specificity = 0.88\n",
    "- Test set\n",
    "    - Accuracy = 0.86\n",
    "    - Sensitivity = 0.70\n",
    "    - Specificity = 0.87\n",
    "    \n",
    "    \n",
    "We can see from the model performance that the Sesitivity has been decreased while evaluating the model on the test set. However, the accuracy and specificity is quite good in the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ab555f",
   "metadata": {},
   "source": [
    "## Random forest with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e06521f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing random forest classifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59919b8",
   "metadata": {},
   "source": [
    "##### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ce1541",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'max_depth': range(5,10,5),\n",
    "    'min_samples_leaf': range(50, 150, 50),\n",
    "    'min_samples_split': range(50, 150, 50),\n",
    "    'n_estimators': [100,200,300], \n",
    "    'max_features': [10, 20]\n",
    "}\n",
    "# Create a based model\n",
    "rf = RandomForestClassifier()\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = rf, \n",
    "                           param_grid = param_grid, \n",
    "                           cv = 3,\n",
    "                           n_jobs = -1,\n",
    "                           verbose = 1, \n",
    "                           return_train_score=True)\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_train_pca, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6c0cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the optimal accuracy score and hyperparameters\n",
    "print('We can get accuracy of',grid_search.best_score_,'using',grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb5c0a4",
   "metadata": {},
   "source": [
    "##### Model with optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc3889f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model with the best hyperparameters\n",
    "\n",
    "rfc_model = RandomForestClassifier(bootstrap=True,\n",
    "                             max_depth=5,\n",
    "                             min_samples_leaf=50, \n",
    "                             min_samples_split=100,\n",
    "                             max_features=20,\n",
    "                             n_estimators=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945e3dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "rfc_model.fit(X_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f2b806",
   "metadata": {},
   "source": [
    "##### Prediction on the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc689687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on the train set\n",
    "y_train_pred = rfc_model.predict(X_train_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4817cf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "confusion = metrics.confusion_matrix(y_train, y_train_pred)\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff06ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = confusion[1,1] # true positive \n",
    "TN = confusion[0,0] # true negatives\n",
    "FP = confusion[0,1] # false positives\n",
    "FN = confusion[1,0] # false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95e64a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "print(\"Accuracy:-\",metrics.accuracy_score(y_train, y_train_pred))\n",
    "\n",
    "# Sensitivity\n",
    "print(\"Sensitivity:-\",TP / float(TP+FN))\n",
    "\n",
    "# Specificity\n",
    "print(\"Specificity:-\", TN / float(TN+FP))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1741507c",
   "metadata": {},
   "source": [
    "##### Prediction on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4bb382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on the test set\n",
    "y_test_pred = rfc_model.predict(X_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79df344b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "confusion = metrics.confusion_matrix(y_test, y_test_pred)\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae475ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = confusion[1,1] # true positive \n",
    "TN = confusion[0,0] # true negatives\n",
    "FP = confusion[0,1] # false positives\n",
    "FN = confusion[1,0] # false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4afca83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "print(\"Accuracy:-\",metrics.accuracy_score(y_test, y_test_pred))\n",
    "\n",
    "# Sensitivity\n",
    "print(\"Sensitivity:-\",TP / float(TP+FN))\n",
    "\n",
    "# Specificity\n",
    "print(\"Specificity:-\", TN / float(TN+FP))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2698cb3",
   "metadata": {},
   "source": [
    "***Model summary***\n",
    "\n",
    "- Train set\n",
    "    - Accuracy = 0.84\n",
    "    - Sensitivity = 0.88\n",
    "    - Specificity = 0.80\n",
    "- Test set\n",
    "    - Accuracy = 0.80\n",
    "    - Sensitivity = 0.75\n",
    "    - Specificity = 0.80\n",
    "    \n",
    "    \n",
    "We can see from the model performance that the Sesitivity has been decreased while evaluating the model on the test set. However, the accuracy and specificity is quite good in the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3651bff",
   "metadata": {},
   "source": [
    "### Final conclusion with PCA\n",
    "After trying several models we can see that for acheiving the best sensitivity, which was our ultimate goal, the classic Logistic regression or the SVM models preforms well. For both the models the sensitivity was approx 81%. Also we have good accuracy of apporx 85%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c4ad6c",
   "metadata": {},
   "source": [
    "### Without PCA\n",
    "### Logistic regression with No PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd673436",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Importing stats model\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5635ee36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "# Adding the constant to X_train\n",
    "log_no_pca = sm.GLM(y_train,(sm.add_constant(X_train)), family=sm.families.Binomial())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93042ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "log_no_pca = log_no_pca.fit().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264d3621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "log_no_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a50e383",
   "metadata": {},
   "source": [
    "***Model analysis***\n",
    "1. We can see that there are few features have positive coefficients and few have negative.\n",
    "2. Many features have higher p-values and hence became insignificant in the model.\n",
    "\n",
    "***Coarse tuning (Auto+Manual)***\n",
    "\n",
    "We'll first eliminate a few features using Recursive Feature Elimination (RFE), and once we have reached a small set of variables to work with, we can then use manual feature elimination (i.e. manually eliminating features based on observing the p-values and VIFs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c07396",
   "metadata": {},
   "source": [
    "### Feature Selection Using RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d148f9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing logistic regression from sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Intantiate the logistic regression\n",
    "logreg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401ba59b",
   "metadata": {},
   "source": [
    "#### RFE with 15 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff19b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing RFE\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# Intantiate RFE with 15 columns\n",
    "rfe = RFE(logreg, 15)\n",
    "\n",
    "# Fit the rfe model with train set\n",
    "rfe = rfe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bd3df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RFE selected columns\n",
    "rfe_cols = X_train.columns[rfe.support_]\n",
    "print(rfe_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f014f3",
   "metadata": {},
   "source": [
    "### Model-1 with RFE selected columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff143dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding constant to X_train\n",
    "X_train_sm_1 = sm.add_constant(X_train[rfe_cols])\n",
    "\n",
    "#Instantiate the model\n",
    "log_no_pca_1 = sm.GLM(y_train, X_train_sm_1, family=sm.families.Binomial())\n",
    "\n",
    "# Fit the model\n",
    "log_no_pca_1 = log_no_pca_1.fit()\n",
    "\n",
    "log_no_pca_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0753c3fe",
   "metadata": {},
   "source": [
    "#### Checking VIFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efaa681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for the VIF values of the feature variables. \n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c749e265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\n",
    "vif = pd.DataFrame()\n",
    "vif['Features'] = X_train[rfe_cols].columns\n",
    "vif['VIF'] = [variance_inflation_factor(X_train[rfe_cols].values, i) for i in range(X_train[rfe_cols].shape[1])]\n",
    "vif['VIF'] = round(vif['VIF'], 2)\n",
    "vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "vif"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192ac71d",
   "metadata": {},
   "source": [
    "##### Removing column og_others_8, which is insignificant as it has the highest p-value 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810df2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing og_others_8 column \n",
    "log_cols = rfe_cols.to_list()\n",
    "log_cols.remove('og_others_8')\n",
    "print(log_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29949768",
   "metadata": {},
   "source": [
    "### Model-2\n",
    "Building the model after removing og_others_8 variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7742ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding constant to X_train\n",
    "X_train_sm_2 = sm.add_constant(X_train[log_cols])\n",
    "\n",
    "#Instantiate the model\n",
    "log_no_pca_2 = sm.GLM(y_train, X_train_sm_2, family=sm.families.Binomial())\n",
    "\n",
    "# Fit the model\n",
    "log_no_pca_2 = log_no_pca_2.fit()\n",
    "\n",
    "log_no_pca_2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b048a67",
   "metadata": {},
   "source": [
    "#### Checking VIF for Model-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb6dd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\n",
    "vif = pd.DataFrame()\n",
    "vif['Features'] = X_train[log_cols].columns\n",
    "vif['VIF'] = [variance_inflation_factor(X_train[log_cols].values, i) for i in range(X_train[log_cols].shape[1])]\n",
    "vif['VIF'] = round(vif['VIF'], 2)\n",
    "vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "vif"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b178a3",
   "metadata": {},
   "source": [
    "As we can see from the model summary that all the variables p-values are significant and offnet_mou_8 column has the highest VIF 7.45. Hence, deleting offnet_mou_8 column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf6bee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing offnet_mou_8 column\n",
    "log_cols.remove('offnet_mou_8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467f2f8b",
   "metadata": {},
   "source": [
    "### Model-3\n",
    "Model after removing offnet_mou_8 column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4bef79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding constant to X_train\n",
    "X_train_sm_3 = sm.add_constant(X_train[log_cols])\n",
    "\n",
    "#Instantiate the model\n",
    "log_no_pca_3 = sm.GLM(y_train, X_train_sm_3, family=sm.families.Binomial())\n",
    "\n",
    "# Fit the model\n",
    "log_no_pca_3 = log_no_pca_3.fit()\n",
    "\n",
    "log_no_pca_3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55a04a7",
   "metadata": {},
   "source": [
    "#### VIF Model-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06a0570",
   "metadata": {},
   "outputs": [],
   "source": [
    "vif = pd.DataFrame()\n",
    "vif['Features'] = X_train[log_cols].columns\n",
    "vif['VIF'] = [variance_inflation_factor(X_train[log_cols].values, i) for i in range(X_train[log_cols].shape[1])]\n",
    "vif['VIF'] = round(vif['VIF'], 2)\n",
    "vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "vif"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee57cb5",
   "metadata": {},
   "source": [
    "Now from the model summary and the VIF list we can see that all the variables are significant and there is no multicollinearity among the variables.\n",
    "\n",
    "Hence, we can conclused that ***Model-3 log_no_pca_3 will be the final model***."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be294e1c",
   "metadata": {},
   "source": [
    "###  Model performance on the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab378dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the predicted value on the train set\n",
    "y_train_pred_no_pca = log_no_pca_3.predict(X_train_sm_3)\n",
    "y_train_pred_no_pca.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768882f9",
   "metadata": {},
   "source": [
    "##### Creating a dataframe with the actual churn and the predicted probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68db050",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_final = pd.DataFrame({'churn':y_train.values, 'churn_prob':y_train_pred_no_pca.values})\n",
    "\n",
    "#Assigning Customer ID for each record for better readblity\n",
    "#CustID is the index of each record.\n",
    "y_train_pred_final['CustID'] = y_train_pred_final.index\n",
    "\n",
    "y_train_pred_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dbdb37",
   "metadata": {},
   "source": [
    "##### Finding Optimal Probablity Cutoff Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22166f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating columns for different probablity cutoffs\n",
    "prob_cutoff = [float(p/10) for p in range(10)]\n",
    "\n",
    "for i in prob_cutoff:\n",
    "    y_train_pred_final[i] = y_train_pred_final['churn_prob'].map(lambda x : 1 if x > i else 0)\n",
    "    \n",
    "y_train_pred_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a69ce5",
   "metadata": {},
   "source": [
    "##### Now let's calculate the accuracy sensitivity and specificity for various probability cutoffs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deabe304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe\n",
    "cutoff_df = pd.DataFrame(columns=['probability', 'accuracy', 'sensitivity', 'specificity'])\n",
    "\n",
    "for i in prob_cutoff:\n",
    "    cm1 = metrics.confusion_matrix(y_train_pred_final['churn'], y_train_pred_final[i] )\n",
    "    total1=sum(sum(cm1))\n",
    "    accuracy = (cm1[0,0]+cm1[1,1])/total1\n",
    "    \n",
    "    speci = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "    sensi = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
    "    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\n",
    "print(cutoff_df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d873ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting accuracy, sensitivity and specificity for different probabilities.\n",
    "cutoff_df.plot('probability', ['accuracy','sensitivity','specificity'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c406f84",
   "metadata": {},
   "source": [
    "##### Analysis of the above curve\n",
    "Accuracy - Becomes stable around 0.6\n",
    "\n",
    "Sensitivity - Decreases with the increased probablity.\n",
    "\n",
    "Specificity - Increases with the increasing probablity.\n",
    "\n",
    "`At point 0.6` where the three parameters cut each other, we can see that there is a balance bethween sensitivity and specificity with a good accuracy.\n",
    "\n",
    "Here we are intended to acheive better sensitivity than accuracy and specificity. Though as per the above curve, we should take 0.6 as the optimum probability cutoff, we are taking ***0.5*** for acheiving higher sensitivity, which is our main goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dbdfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a column with name \"predicted\", which is the predicted value for 0.5 cutoff \n",
    "y_train_pred_final['predicted'] = y_train_pred_final['churn_prob'].map(lambda x: 1 if x > 0.5 else 0)\n",
    "y_train_pred_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319eba2d",
   "metadata": {},
   "source": [
    "##### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986be48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion metrics\n",
    "confusion = metrics.confusion_matrix(y_train_pred_final['churn'], y_train_pred_final['predicted'])\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15d0870",
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = confusion[1,1] # true positive \n",
    "TN = confusion[0,0] # true negatives\n",
    "FP = confusion[0,1] # false positives\n",
    "FN = confusion[1,0] # false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded2efe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "print(\"Accuracy:-\",metrics.accuracy_score(y_train_pred_final['churn'], y_train_pred_final['predicted']))\n",
    "\n",
    "# Sensitivity\n",
    "print(\"Sensitivity:-\",TP / float(TP+FN))\n",
    "\n",
    "# Specificity\n",
    "print(\"Specificity:-\", TN / float(TN+FP))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879088f8",
   "metadata": {},
   "source": [
    "We have got good accuracy, sensitivity and specificity on the train set prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3453b833",
   "metadata": {},
   "source": [
    "##### Plotting the ROC Curve (Trade off between sensitivity & specificity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c879e263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve function\n",
    "\n",
    "def draw_roc( actual, probs ):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n",
    "                                              drop_intermediate = False )\n",
    "    auc_score = metrics.roc_auc_score( actual, probs )\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e32340",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_roc(y_train_pred_final['churn'], y_train_pred_final['churn_prob'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f184e84",
   "metadata": {},
   "source": [
    "We can see the area of the ROC curve is closer to 1, whic is the Gini of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5b8b27",
   "metadata": {},
   "source": [
    "### Testing the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1e65c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking a copy of the test set\n",
    "X_test_log = X_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c5a03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking only the columns, which are selected in the train set after removing insignificant and multicollinear variables\n",
    "X_test_log = X_test_log[log_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9daaf63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding constant on the test set\n",
    "X_test_sm = sm.add_constant(X_test_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da3925a",
   "metadata": {},
   "source": [
    "##### Predictions on the test set with final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c87ba1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "y_test_pred = log_no_pca_3.predict(X_test_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c58862d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03a520b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting y_test_pred to a dataframe because y_test_pred is an array\n",
    "y_pred_1 = pd.DataFrame(y_test_pred)\n",
    "y_pred_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6d598e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convetting y_test to a dataframe\n",
    "y_test_df = pd.DataFrame(y_test)\n",
    "y_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cc3eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting index to Customer ID \n",
    "y_test_df['CustID'] = y_test_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc138b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing index form the both dataframes for merging them side by side\n",
    "y_pred_1.reset_index(drop=True, inplace=True)\n",
    "y_test_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71706f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appending y_pred_1 and y_test_df\n",
    "y_test_pred_final = pd.concat([y_test_df, y_pred_1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6255cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469b3d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the '0' column as churn probablity\n",
    "y_test_pred_final = y_test_pred_final.rename(columns={0:'churn_prob'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a267c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearranging the columns\n",
    "y_test_pred_final = y_test_pred_final.reindex_axis(['CustID','churn','churn_prob'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c09283d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbecebc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the test set using probablity cutoff 0.5, what we got in the train set \n",
    "y_test_pred_final['test_predicted'] = y_test_pred_final['churn_prob'].map(lambda x: 1 if x > 0.5 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01925bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c492e06",
   "metadata": {},
   "source": [
    "##### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c732fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "confusion = metrics.confusion_matrix(y_test_pred_final['churn'], y_test_pred_final['test_predicted'])\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7f3afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = confusion[1,1] # true positive \n",
    "TN = confusion[0,0] # true negatives\n",
    "FP = confusion[0,1] # false positives\n",
    "FN = confusion[1,0] # false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f972623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "print(\"Accuracy:-\",metrics.accuracy_score(y_test_pred_final['churn'], y_test_pred_final['test_predicted']))\n",
    "\n",
    "# Sensitivity\n",
    "print(\"Sensitivity:-\",TP / float(TP+FN))\n",
    "\n",
    "# Specificity\n",
    "print(\"Specificity:-\", TN / float(TN+FP))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ec06e4",
   "metadata": {},
   "source": [
    "***Model summary***\n",
    "\n",
    "- Train set\n",
    "    - Accuracy = 0.84\n",
    "    - Sensitivity = 0.81\n",
    "    - Specificity = 0.83\n",
    "- Test set\n",
    "    - Accuracy = 0.78\n",
    "    - Sensitivity = 0.82\n",
    "    - Specificity = 0.78\n",
    "    \n",
    "Overall, the model is performing well in the test set, what it had learnt from the train set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6c1b8b",
   "metadata": {},
   "source": [
    "#### Final conclusion with no PCA\n",
    "\n",
    "We can see that the logistic model with no PCA has good sensitivity and accuracy, which are comparable to the models with PCA. So, we can go for the more simplistic model such as logistic regression with PCA as it expliains the important predictor variables as well as the significance of each variable. The model also hels us to identify the variables which should be act upon for making the decision of the to be churned customers. Hence, the model is more relevant in terms of explaining to the business."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6913952a",
   "metadata": {},
   "source": [
    "## Business recomendation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0b5b79",
   "metadata": {},
   "source": [
    "#### Top predictors\n",
    "\n",
    "Below are few top variables selected in the logistic regression model.\n",
    "\n",
    "| Variables   | Coefficients |\n",
    "|---------------------|--------------|\n",
    "|loc_ic_mou_8|-3.3287|\n",
    "|og_others_7|-2.4711|\n",
    "|ic_others_8|-1.5131|\n",
    "|isd_og_mou_8|-1.3811|\n",
    "|decrease_vbc_action|-1.3293|\n",
    "|monthly_3g_8|-1.0943|\n",
    "|std_ic_t2f_mou_8|-0.9503|\n",
    "|monthly_2g_8|-0.9279|\n",
    "|loc_ic_t2f_mou_8|-0.7102|\n",
    "|roam_og_mou_8|0.7135|\n",
    "\n",
    "We can see most of the top variables have negative coefficients. That means, the variables are inversely correlated with the churn probablity.\n",
    "\n",
    "E.g.:- \n",
    "\n",
    "If the local incoming minutes of usage (loc_ic_mou_8) is lesser in the month of August than any other month, then there is a higher chance that the customer is likely to churn.\n",
    "\n",
    "***Recomendations***\n",
    "\n",
    "1. Target the customers, whose minutes of usage of the incoming local calls and outgoing ISD calls are less in the action phase (mostly in the month of August).\n",
    "2. Target the customers, whose outgoing others charge in July and incoming others on August are less.\n",
    "3. Also, the customers having value based cost in the action phase increased are more likely to churn than the other customers. Hence, these customers may be a good target to provide offer.\n",
    "4. Cutomers, whose monthly 3G recharge in August is more, are likely to be churned. \n",
    "5. Customers having decreasing STD incoming minutes of usage for operators T to fixed lines of T for the month of August are more likely to churn.\n",
    "6. Cutomers decreasing monthly 2g usage for August are most probable to churn.\n",
    "7. Customers having decreasing incoming minutes of usage for operators T to fixed lines of T for August are more likely to churn.\n",
    "8. roam_og_mou_8 variables have positive coefficients (0.7135). That means for the customers, whose roaming outgoing minutes of usage is increasing are more likely to churn.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4384ce65",
   "metadata": {},
   "source": [
    "#### Plots of important predictors for churn and non churn customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18acea39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting loc_ic_mou_8 predictor for churn and not churn customers\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "sns.distplot(data_churn['loc_ic_mou_8'],label='churn',hist=False)\n",
    "sns.distplot(data_non_churn['loc_ic_mou_8'],label='not churn',hist=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ab8e50",
   "metadata": {},
   "source": [
    "We can see that for the churn customers the minutes of usage for the month of August is mostly populated on the lower side than the non churn customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c334a031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting isd_og_mou_8 predictor for churn and not churn customers\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "sns.distplot(data_churn['isd_og_mou_8'],label='churn',hist=False)\n",
    "sns.distplot(data_non_churn['isd_og_mou_8'],label='not churn',hist=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3580304a",
   "metadata": {},
   "source": [
    "We can see that the ISD outgoing minutes of usage for the month of August for churn customers is densed approximately to zero. On the onther hand for the non churn customers it is little more than the churn customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c194c149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting monthly_3g_8 predictor for churn and not churn customers\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "sns.distplot(data_churn['monthly_3g_8'],label='churn',hist=False)\n",
    "sns.distplot(data_non_churn['monthly_3g_8'],label='not churn',hist=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28517076",
   "metadata": {},
   "source": [
    "The number of mothly 3g data for August for the churn customers are very much populated aroud 1, whereas of non churn customers it spreaded accross various numbers.\n",
    "\n",
    "Similarly we can plot each variables, which have higher coefficients, churn distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c491fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
